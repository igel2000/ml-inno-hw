{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a17b58",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4a46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, padding=2),   # [28x28x6]\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),       # [14x14x6]\n",
    "            nn.Conv2d(6, 16, kernel_size=5),             # [10x10x16]\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)        # [5x5x16]\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(5*5*16, 120),                      # [120]\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),                          # [84]\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)                            # [10]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)        # [400]  \n",
    "        x = self.fc_layers(x)\n",
    "        x = F.log_softmax(x, dim=1)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc961b05",
   "metadata": {},
   "source": [
    "# learning_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d380fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_function(model, train_loader, test_loader, \n",
    "                      epochs=10, learning_rate=0.001, device='auto'):\n",
    "    \n",
    "    metrics = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    # Определение устройства\n",
    "    if device == 'auto':\n",
    "        device_for_learning = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif device == 'cpu':\n",
    "        device_for_learning = torch.device(\"cpu\")\n",
    "    elif device == 'gpu':\n",
    "        device_for_learning = torch.device(\"gpu\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid device choice. Use 'auto', 'cpu', or 'gpu'.\")\n",
    "\n",
    "    model.to(device_for_learning)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Не знаем что взять - пробуем Adam\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Планировщик скорости обучения\n",
    "\n",
    "    # Цикл обучения\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for data, target in train_loader: #tqdm_notebook(train_loader, desc='Train', leave=False): \n",
    "            data, target = data.to(device_for_learning), target.to(device_for_learning)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.inference_mode():\n",
    "            for data, target in test_loader: #tqdm_notebook(test_loader, desc='Train', leave=False): \n",
    "                data, target = data.to(device_for_learning), target.to(device_for_learning)\n",
    "                logits = model(data)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(train_loader):.4f} | \"\n",
    "            f\"LR: {scheduler.get_last_lr()[0]:.5f} | \"\n",
    "            f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Сохранение модели\n",
    "    # torch.save(model.state_dict(), \"lenet_fashionmnist.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ad5d",
   "metadata": {},
   "source": [
    "# Загрузка датасета, настройка параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b50689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.005\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Трансформации для данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() #,\n",
    "    #transforms.Normalize((0.2860,), (0.3530,))  # Статистика FashionMNIST\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225916a0",
   "metadata": {},
   "source": [
    "### Реализация DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676cdf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка FashionMNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9814b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Loss: 0.9809 | LR: 0.00500 | Test Accuracy: 74.34%\n",
      "Epoch [2/15] Loss: 0.5334 | LR: 0.00500 | Test Accuracy: 81.81%\n",
      "Epoch [3/15] Loss: 0.4361 | LR: 0.00500 | Test Accuracy: 84.38%\n",
      "Epoch [4/15] Loss: 0.3835 | LR: 0.00500 | Test Accuracy: 85.20%\n",
      "Epoch [5/15] Loss: 0.3503 | LR: 0.00250 | Test Accuracy: 86.56%\n",
      "Epoch [6/15] Loss: 0.3143 | LR: 0.00250 | Test Accuracy: 86.71%\n",
      "Epoch [7/15] Loss: 0.3042 | LR: 0.00250 | Test Accuracy: 87.63%\n",
      "Epoch [8/15] Loss: 0.2940 | LR: 0.00250 | Test Accuracy: 87.98%\n",
      "Epoch [9/15] Loss: 0.2871 | LR: 0.00250 | Test Accuracy: 87.87%\n",
      "Epoch [10/15] Loss: 0.2769 | LR: 0.00125 | Test Accuracy: 88.30%\n",
      "Epoch [11/15] Loss: 0.2607 | LR: 0.00125 | Test Accuracy: 88.38%\n",
      "Epoch [12/15] Loss: 0.2558 | LR: 0.00125 | Test Accuracy: 88.69%\n",
      "Epoch [13/15] Loss: 0.2523 | LR: 0.00125 | Test Accuracy: 88.89%\n",
      "Epoch [14/15] Loss: 0.2480 | LR: 0.00125 | Test Accuracy: 89.00%\n",
      "Epoch [15/15] Loss: 0.2446 | LR: 0.00063 | Test Accuracy: 88.82%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "learning_function(model, train_loader, test_loader, epochs=15, learning_rate=0.005, device='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf81dfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Loss: 1.0908 | LR: 0.00500 | Test Accuracy: 75.12%\n",
      "Epoch [2/15] Loss: 0.5515 | LR: 0.00500 | Test Accuracy: 80.30%\n",
      "Epoch [3/15] Loss: 0.4490 | LR: 0.00500 | Test Accuracy: 82.29%\n",
      "Epoch [4/15] Loss: 0.4033 | LR: 0.00500 | Test Accuracy: 84.48%\n",
      "Epoch [5/15] Loss: 0.3757 | LR: 0.00250 | Test Accuracy: 85.52%\n",
      "Epoch [6/15] Loss: 0.3344 | LR: 0.00250 | Test Accuracy: 86.18%\n",
      "Epoch [7/15] Loss: 0.3230 | LR: 0.00250 | Test Accuracy: 86.78%\n",
      "Epoch [8/15] Loss: 0.3130 | LR: 0.00250 | Test Accuracy: 87.04%\n",
      "Epoch [9/15] Loss: 0.3033 | LR: 0.00250 | Test Accuracy: 87.02%\n",
      "Epoch [10/15] Loss: 0.2965 | LR: 0.00125 | Test Accuracy: 87.10%\n",
      "Epoch [11/15] Loss: 0.2782 | LR: 0.00125 | Test Accuracy: 87.91%\n",
      "Epoch [12/15] Loss: 0.2738 | LR: 0.00125 | Test Accuracy: 88.23%\n",
      "Epoch [13/15] Loss: 0.2690 | LR: 0.00125 | Test Accuracy: 88.20%\n",
      "Epoch [14/15] Loss: 0.2648 | LR: 0.00125 | Test Accuracy: 87.63%\n",
      "Epoch [15/15] Loss: 0.2617 | LR: 0.00063 | Test Accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "learning_function(model, train_loader, test_loader, epochs=15, learning_rate=0.005, device='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266f16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "torch.Size([60000, 28, 28]) tensor(3431114169)\n",
      "Mean: 0.2860, Std: 0.3530\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset)\n",
    "print(test_loader.dataset)\n",
    "print(train_loader.dataset.data.shape, train_loader.dataset.data.sum())\n",
    "\n",
    "# Сбор всех данных в один тензор\n",
    "data = torch.cat([img for img, _ in train_loader.dataset], dim=0)\n",
    "\n",
    "# Расчет среднего и std\n",
    "mean = data.mean().item()\n",
    "std = data.std().item()\n",
    "\n",
    "print(f\"Mean: {mean:.4f}, Std: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871097ca",
   "metadata": {},
   "source": [
    "### Реализация HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2e81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "fashion_mnist_dataset = datasets.FashionMNIST(root=\"fashion\",  \n",
    "                                              train=True,\n",
    "                                              download=True, \n",
    "                                              transform=transform)\n",
    "# Содержит 60000 семплов и 10 классов\n",
    "images = fashion_mnist_dataset.data.view([60000, 1, 28, 28]).float()\n",
    "\n",
    "labels = deepcopy(fashion_mnist_dataset.targets)# - 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.01)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "batch_size = 128\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size=len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0006f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: fashion\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "torch.Size([60000, 28, 28]) tensor(3431114169)\n",
      "Mean: 0.2860, Std: 0.3530\n"
     ]
    }
   ],
   "source": [
    "print(fashion_mnist_dataset)\n",
    "print(fashion_mnist_dataset.data.shape, fashion_mnist_dataset.data.sum())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Сбор всех данных в один тензор\n",
    "data = torch.cat([img for img, _ in fashion_mnist_dataset], dim=0)\n",
    "\n",
    "# Расчет среднего и std\n",
    "mean = data.mean().item()\n",
    "std = data.std().item()\n",
    "\n",
    "print(f\"Mean: {mean:.4f}, Std: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9ff93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e6a288f3184fc998696231689d6474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03edeb48be145e4b067261d37f0cf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Loss: 0.7897 | LR: 0.00500 | Test Accuracy: 78.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7113cb8c55ec4c96bc14db87fbada1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333951c3050442485842a9f565d73c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Loss: 0.4517 | LR: 0.00500 | Test Accuracy: 81.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6ef8173f6e433f963ffcce265e1d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cef6db6d8784c2fa0ae8dd586ca342b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] Loss: 0.3839 | LR: 0.00500 | Test Accuracy: 86.33%\n"
     ]
    }
   ],
   "source": [
    "model2 = LeNet()\n",
    "learning_function(model2, train_dl, val_dl, epochs=3, learning_rate=0.005, device='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661719d",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e0a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LeNet(how_inizialize='relu')\n",
    "#learning_function(model, train_loader, test_loader, epochs=10, learning_rate=0.005, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe8f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LeNet(how_inizialize='sigmoid')\n",
    "#learning_function(model, train_loader, test_loader, epochs=10, learning_rate=0.005, device='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
