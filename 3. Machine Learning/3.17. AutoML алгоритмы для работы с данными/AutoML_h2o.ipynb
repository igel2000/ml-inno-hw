{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Импорты и библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from pprint import pprint, pformat\n",
    "import zipfile\n",
    "\n",
    "import opendatasets as od \n",
    "import pandas as pd\n",
    "import pandas.api.types as pd_types\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as plotly_px\n",
    "import plotly.graph_objects as plotly_go\n",
    "import plotly.subplots as plotly_subplt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import joblib\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import builtins\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    train_test_split,  # Функция для разделения данных на обучающую и тестовую выборки\n",
    "    cross_val_score, # оценщик кросс-валидации\n",
    "    GridSearchCV,  # Класс для поиска гиперпараметров с помощью сеточного поиска\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (OneHotEncoder, \n",
    "                                   OrdinalEncoder\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLib():\n",
    "    @staticmethod\n",
    "    def st():\n",
    "        \"\"\"старт таймера\"\"\"\n",
    "        return time.monotonic_ns()\n",
    "    @staticmethod\n",
    "    def ft(start):\n",
    "        \"\"\"финиш таймера и вывод времени\"\"\"\n",
    "        duration = (time.monotonic_ns() - start) / 1000000000\n",
    "        print(f'Затрачено времени: {duration:.2f} секунд')\n",
    "        return duration\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type(type_name):\n",
    "        try:\n",
    "            return getattr(builtins, type_name)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                obj = globals()[type_name]\n",
    "            except KeyError:\n",
    "                return None\n",
    "            return repr(obj) if isinstance(obj, type) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSetLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetLib():\n",
    "    \"\"\"Библиотека функций для работы с датасетом\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def columns_by_type(df, target_name, cat_treshold=2):\n",
    "        \"\"\"Сфромировать словарь, с разделением имен столбцов по типам:\n",
    "        - target_columns - целевой столбец\n",
    "        - columns_X - все столбцы-фичи\n",
    "        - num_columns - числовые столбцы\n",
    "        - cat_columns - категориальные. Категориальными считаютс и числовые столбцы, в которых уникальных значений меньше или равно  cat_treshold\"\"\"\n",
    "        params = {}\n",
    "        # columns_X - переменные датасета\n",
    "        params[\"columns_X\"] = df.columns.to_list()\n",
    "        # целевой столбец\n",
    "        params[\"target_column\"] = target_name\n",
    "        if target_name is not None:\n",
    "            params[\"columns_X\"].remove(params[\"target_column\"])\n",
    "        params[\"num_columns\"] = []\n",
    "        params[\"cat_columns\"] = []\n",
    "        # определить числовые и категориальные столбцы\n",
    "        for col in params[\"columns_X\"]:\n",
    "            if df[col].nunique() <= cat_treshold or not pd_types.is_numeric_dtype(df[col]):\n",
    "                params[\"cat_columns\"].append(col)\n",
    "            else:\n",
    "                params[\"num_columns\"].append(col)\n",
    "        #print(f'target_columns={params[\"target_column\"]}')        \n",
    "        #print(f'columns_X={params[\"columns_X\"]}')\n",
    "        #print(f'cat_columns={pformat(params[\"cat_columns\"])}')\n",
    "        #print(f'num_columns={pformat(params[\"num_columns\"])}')\n",
    "        return params\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_columns(params, column): \n",
    "        \"\"\"удалить столбец и словаря параметров\"\"\"\n",
    "        if column in params[\"columns_X\"]:\n",
    "            params[\"columns_X\"].remove(column)\n",
    "        if column in params[\"num_columns\"]:\n",
    "            params[\"num_columns\"].remove(column)\n",
    "        if column in params[\"cat_columns\"]:\n",
    "            params[\"cat_columns\"].remove(column)\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def add_columns(params, column, type_column): \n",
    "        \"\"\"добавить столбец в словарь параметров\n",
    "        type_column = \"cat\" или \"num\" или None \"\"\"\n",
    "        if column not in params[\"columns_X\"]:\n",
    "            params[\"columns_X\"].append(column)\n",
    "        if type_column is None:\n",
    "            pass\n",
    "        elif type_column == \"cat\":\n",
    "            params[\"cat_columns\"].append(column)\n",
    "        elif type_column == \"num\":\n",
    "            params[\"num_columns\"].append(column)\n",
    "        else:\n",
    "            raise ValueError(\"type_column должен быть 'cat' или 'num'\")\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def describe_columns(df, params):\n",
    "        \"\"\"Отобразить описание содержимого столбцов\"\"\"    \n",
    "        # подсчет столбцов с пропусками\n",
    "        nan_in_columns = DataSetLib.nans_percents(df)\n",
    "        \n",
    "        print(\"Количество уникальных значений по столбцам, доля пропусков и уникальные значения, если их не более 10\")\n",
    "        nunique = df[params[\"columns_X\"]].nunique()\n",
    "        for column in nunique.index:\n",
    "            if column in params[\"cat_columns\"]:\n",
    "                column_type = \"[c]\" # категориальные\n",
    "            else:\n",
    "                column_type = \"[n]\" # числовые\n",
    "                \n",
    "            if nan_in_columns[column] > 0:\n",
    "                nan_str = f'({nan_in_columns[column]:4.1f}%)'\n",
    "            else:\n",
    "                nan_str = \" \"*7\n",
    "            if nunique[column] <= 10:\n",
    "                print(f'{column:20}{column_type}: {nunique[column]:6} {nan_str}, {df[column].unique().tolist()}')\n",
    "            else:\n",
    "                print(f'{column:20}{column_type}: {nunique[column]:6} {nan_str}')\n",
    "\n",
    "        if params[\"target_column\"] is not None:\n",
    "            df_describe_num = DataSetLib.eda_df(df[params[\"num_columns\"]+[params[\"target_column\"]]])\n",
    "        else:\n",
    "            df_describe_num = DataSetLib.eda_df(df[params[\"num_columns\"]])\n",
    "        display(df_describe_num)            \n",
    "\n",
    "        df_describe_cat = df[params[\"cat_columns\"]].describe()\n",
    "        display(df_describe_cat)\n",
    "\n",
    "    @staticmethod\n",
    "    def eda_df(df):\n",
    "        \"\"\"Провести EDA для датафрейма\"\"\"\n",
    "        df_describe = df.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        # посчитать долю пропусков\n",
    "        df_describe.loc[\"%nan\"] = (np.round(df[df_describe.columns].isna().mean()*100, 2)).to_list()\n",
    "        # посчитать дисперсию\n",
    "        columns_var = []\n",
    "        for column in df_describe.columns:\n",
    "            columns_var.append(df[column].var())\n",
    "        df_describe.loc['var'] = columns_var\n",
    "        return df_describe\n",
    "\n",
    "    @staticmethod\n",
    "    def show_boxes(df, columns, ncols = 3, type=\"box\", row_height=500, total_width=1200):\n",
    "        \"\"\"Показать 'ящики_с_усами' для набора df.\n",
    "        Ящики будут показаны для столбцов датафрема, перечисленных в columns.\n",
    "        Графики будут показаны в несколько столбцов, количество которых задается в параметре ncols.\"\"\"\n",
    "        nrows = int(round((len(columns) + 0.5) / ncols, 0))\n",
    "        nrows = nrows if nrows > 1 else 1\n",
    "\n",
    "        if type == \"box\":\n",
    "            title = \"Ящики с усами\"\n",
    "        elif type == \"hist\":\n",
    "            title = \"Гистрограммы\"\n",
    "        elif type == \"pie\":\n",
    "            title = \"Пирожки\"\n",
    "        else:\n",
    "            raise f\"Не реализована обработка типа графика {type}\"\n",
    "\n",
    "\n",
    "        fig = plotly_subplt.make_subplots(rows=nrows, cols=ncols)\n",
    "        fig.update_layout(\n",
    "            title_x=0.5,\n",
    "            title_text=title,\n",
    "            height=row_height*nrows, \n",
    "            width=total_width\n",
    "        )\n",
    "        i = 0\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if type == \"box\":\n",
    "                    fig.add_box(y=df[columns[i]], name=columns[i], row=r+1, col=c+1)\n",
    "                elif type == \"hist\":\n",
    "                    fig.add_histogram(x=df[columns[i]], name=columns[i], row=r+1, col=c+1)\n",
    "                elif type == \"pie\":\n",
    "                    fig.add_pie(df[columns[i]].value_counts().values,\n",
    "                                labels=df[columns[i]].value_counts().index, \n",
    "                                name=columns[i], row=r+1, col=c+1)\n",
    "                else:\n",
    "                    raise f\"Не реализована обработка типа графика {type}\"\n",
    "                i += 1\n",
    "                if i >= len(columns):\n",
    "                    break\n",
    "            if i >= len(columns):\n",
    "                break\n",
    "        fig.show()          \n",
    "\n",
    "    @staticmethod\n",
    "    def show_boxes_plt(df, columns_x, ncols = 3, type=\"box\", row_height=500, total_width=1200, column_y=None, filename=None):\n",
    "        \"\"\"Показать 'ящики_с_усами' для набора df.\n",
    "        Ящики будут показаны для столбцов датафрема, перечисленных в columns.\n",
    "        Графики будут показаны в несколько столбцов, количество которых задается в параметре ncols.\"\"\"\n",
    "        nrows = int(round((len(columns_x) + 0.59) / ncols, 0))\n",
    "        nrows = nrows if nrows > 1 else 1\n",
    "\n",
    "        if type == \"box\":\n",
    "            title = \"Ящики с усами\"\n",
    "        elif type == \"hist\":\n",
    "            title = \"Гистрограммы\"\n",
    "        elif type == \"pie\":\n",
    "            title = \"Пирожки\"\n",
    "        else:\n",
    "            raise f\"Не реализована обработка типа графика {type}\"\n",
    "                \n",
    "        plt.figure(figsize=(ncols * 5, nrows * 3))\n",
    "        \n",
    "        for i, column in enumerate(columns_x, start=1):\n",
    "            plt.subplot(nrows, ncols, i)\n",
    "            if type == \"box\":\n",
    "                if column_y is None:\n",
    "                    sns.boxplot(x=df[column])\n",
    "                else:\n",
    "                    sns.boxplot(x=df[column], y=df[column_y])\n",
    "            elif type == \"hist\":\n",
    "                sns.histplot(df[column], kde=True)\n",
    "            elif type == \"pie\":\n",
    "                # define Seaborn color palette to use \n",
    "                palette_color = sns.color_palette(\"pastel\") \n",
    "                # plotting data on chart \n",
    "                plt.pie(x=df[column].value_counts().values, \n",
    "                        labels=df[column].value_counts().index, \n",
    "                        colors=palette_color, autopct='%.0f%%') \n",
    "            else:\n",
    "                raise f\"Не реализована обработка типа графика {type}\"\n",
    "            # Добавить название столбца как заголовок графика\n",
    "            plt.title(column)\n",
    "        plt.tight_layout()\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, dpi=300)\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "    @staticmethod            \n",
    "    def iqr_values(values):\n",
    "        \"\"\"Границы для ящика-с-усами\n",
    "        Возвращаемые значения: Q1, Q3, IQR, lower, upper\n",
    "        \"\"\"\n",
    "        Q3 = np.quantile(values, 0.75, axis=0)\n",
    "        Q1 = np.quantile(values, 0.25, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        return Q1, Q3, IQR, lower, upper\n",
    "\n",
    "        \n",
    "    @staticmethod    \n",
    "    def nans_percents(df):\n",
    "        return df.isna().sum()/len(df)*100    \n",
    "\n",
    "    @staticmethod\n",
    "    def encode_features(src_df, onehot_cols=None, onehot_drop=None, ordinal_cols=None, columns_X=None):\n",
    "        df = src_df.copy()  \n",
    "        new_columns_X = copy.deepcopy(columns_X)\n",
    "        if onehot_cols is not None:\n",
    "            encoder = OneHotEncoder(sparse_output=False, drop=onehot_drop)\n",
    "            one_hot_encoded = encoder.fit_transform(df[onehot_cols])\n",
    "            one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(onehot_cols))\n",
    "            df = pd.concat([df, one_hot_df], axis=1)\n",
    "            new_columns_X += encoder.get_feature_names_out(onehot_cols).tolist()\n",
    "            for col in onehot_cols:\n",
    "                if col in columns_X:\n",
    "                    new_columns_X.remove(col)\n",
    "            df.drop(onehot_cols, axis=1, inplace=True)\n",
    "            \n",
    "        if ordinal_cols is not None:\n",
    "            ordinal_columns_cats = list(ordinal_cols.values())\n",
    "            ordinal_columns_list = list(ordinal_cols.keys())\n",
    "            encoder = OrdinalEncoder(categories = ordinal_columns_cats)\n",
    "            df[ordinal_columns_list] = encoder.fit_transform(df[ordinal_columns_list])  \n",
    "\n",
    "        return df, new_columns_X\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_with_mode(data, group_col, target_col):\n",
    "        \"\"\"Заполнить target_col модой внутри каждой группы столбцов group_col\"\"\"\n",
    "        global_mode = data[target_col].mode()[0]\n",
    "        def fill_group_mode(x):\n",
    "            group_mode = x.mode()\n",
    "            if not group_mode.empty:\n",
    "                return group_mode[0]\n",
    "            else:\n",
    "                return global_mode\n",
    "        data[target_col] = data.groupby(group_col)[target_col].transform(fill_group_mode)\n",
    "\n",
    "    @staticmethod   \n",
    "    def fill_with_mean(data, group_col, target_col):\n",
    "        \"\"\"Заполнить target_col средним внутри каждой группы столбцов group_col\"\"\"\n",
    "        def fill_group_mean(x):\n",
    "            return x.mean()\n",
    "        data[target_col] = data.groupby(group_col)[target_col].transform(fill_group_mean)    \n",
    "        # заполним глобальным средним, если что-то пропустилось\n",
    "        data.fillna({target_col: data[target_col].mean()}, inplace=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_dataset(dataset_df, params, scaler=None, train_size=0.7):\n",
    "        \"\"\"Разделить датасет на тренировочную и тестовую выборки и прогнать через нормализатор, если он указан\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset_df[params[\"columns_X\"]], \n",
    "                                                            dataset_df[params[\"target_column\"]], \n",
    "                                                            train_size=train_size, \n",
    "                                                            stratify=dataset_df[params[\"target_column\"]],\n",
    "                                                            random_state=42)\n",
    "        # Нормировка признаков\n",
    "        if scaler is not None:\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def find_rows_with_nan(dataset_df, columns, debug=False):\n",
    "        # сначала посмотрим на столбцы с измерениями \n",
    "        all_rows_with_nan = []\n",
    "        rows_with_all_columns_nan = []\n",
    "        for column in columns:\n",
    "            nan_rows = dataset_df[dataset_df[column].isna()].index.to_list()\n",
    "            if debug:\n",
    "                print(f'Индексы строк с пустым {column}: {nan_rows}')\n",
    "            all_rows_with_nan += nan_rows\n",
    "            if rows_with_all_columns_nan == []:\n",
    "                rows_with_all_columns_nan = nan_rows\n",
    "            else:\n",
    "                rows_with_all_columns_nan = list(set(rows_with_all_columns_nan) & set(nan_rows))\n",
    "        # получить уникальный список индексов с пустыми столбцами\n",
    "        all_rows_with_nan = list(set(all_rows_with_nan))    \n",
    "        return rows_with_all_columns_nan, all_rows_with_nan        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class Settings():\n",
    "    enviroment: object\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.dataset_folder = str(Path(Path.cwd(), self.enviroment[\"DATASET_SUBFOLDER\"]))\n",
    "        self.cache_folder = str(Path(Path.cwd(), self.enviroment[\"CACHE_SUBFOLDER\"]))\n",
    "        self.result_folder = str(Path(Path.cwd(), self.enviroment[\"RESULT_SUBFOLDER\"]))\n",
    "        \n",
    "    def cache_gridsearch_filename(self, model_name): \n",
    "        return Path(self.cache_folder, self.enviroment[\"GRID_SEARCH_TEMPLATE_FILENAME\"] % model_name)\n",
    "    def cache_model_filename(self, model_name): \n",
    "        return Path(self.cache_folder, self.enviroment[\"MODEL_CLASS_TEMPLATE_FILENAME\"] % model_name)\n",
    "    def result_gridsearch_filename(self, model_name): \n",
    "        return Path(self.result_folder, self.enviroment[\"GRID_SEARCH_TEMPLATE_FILENAME\"] % model_name) \n",
    "    def result_model_filename(self, model_name): \n",
    "        return Path(self.result_folder, self.enviroment[\"MODEL_CLASS_TEMPLATE_FILENAME\"] % model_name)\n",
    "    def result_trained_model_filename(self, model_name): \n",
    "        return Path(self.result_folder, self.enviroment[\"MODEL_CLASS_TEMPLATE_FILENAME\"] % f'{model_name}_trained')             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelWrapBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapBase(abc.ABC):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.model_params = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self, model_class, model_params, X_train, X_test, y_train, y_test):\n",
    "        self.model_params = model_params\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.model = model_class(**self.model_params)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.model.fit(self.X_train, self.y_train)    \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_metrics(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def show_quality(self): \n",
    "        raise NotImplemented\n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics_names():\n",
    "        raise NotImplemented\n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_or_create_and_fit_model(model_meta_class, \n",
    "                                      model_name, model_class, model_params, \n",
    "                                      X_train, X_test, y_train, y_test,\n",
    "                                      settings, \n",
    "                                      need_save=True):\n",
    "        \"\"\"Загрузить ранее обученную модель из кеша.\n",
    "        Если в кеше нет - обучить на переданных данных с заданными параметрами.\n",
    "        \"\"\"\n",
    "        model_filename_cache = settings.cache_model_filename(model_name)\n",
    "        model_filename = settings.result_model_filename(model_name)\n",
    "\n",
    "        if Path.is_file(model_filename_cache):\n",
    "            model = joblib.load(model_filename_cache)\n",
    "            print(f\"Модель {type(model.model).__name__} загружена из {model_filename_cache}\")\n",
    "        else:\n",
    "            print(f\"Создается и тренируется модель {model_name} класса {type(model_class).__name__}\")\n",
    "            print(f'Гиперпараметры модели: {model_params}')\n",
    "            model = model_meta_class(model_name)\n",
    "            model.create_model(model_class, model_params, X_train, X_test, y_train, y_test)\n",
    "            model.fit()\n",
    "            model.calc_metrics()\n",
    "            if need_save:\n",
    "                print(f\"\\nКласс-обвертка модели сохранен в {model_filename}\")\n",
    "                _ = joblib.dump(model, model_filename)\n",
    "                print(f\"\\nНатренированная модель сохранена в {settings.result_trained_model_filename(model_name)}\")\n",
    "                _= joblib.dump(model.model, settings.result_trained_model_filename(model_name))\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_or_create_and_fit_GridSearchCV(model_name, model_class, param_grid, X_train, y_train,\n",
    "                                             settings, \n",
    "                                             scoring='roc_auc', \n",
    "                                             need_save=True, n_jobs=None, verbose=1,\n",
    "                                             use_randomize_search = True, n_iter=100):\n",
    "        \"\"\"Загрузить ранее обученные GridSearchCV из кеша. \n",
    "        Если в кеше нет - создать и потренировать, найдя лучшие гиперпараметры\"\"\"\n",
    "        \n",
    "        \n",
    "        grid_search_filename_cache = settings.cache_gridsearch_filename(model_name)\n",
    "        grid_search_filename = settings.result_gridsearch_filename(model_name)\n",
    "\n",
    "        if Path.is_file(grid_search_filename_cache):\n",
    "            print(f\"GridSearchCV() загружен из {grid_search_filename_cache}\")\n",
    "            grid_search = joblib.load(grid_search_filename_cache)\n",
    "        else:\n",
    "            if use_randomize_search:\n",
    "                print(f\"Создается и выполняется RandomizedSearchCV для модели {model_name} класса {model_class.__name__}\")\n",
    "                grid_search = RandomizedSearchCV(model_class(), param_grid, cv=5, n_jobs=n_jobs, \n",
    "                                                 verbose=verbose, scoring=scoring,\n",
    "                                                 random_state=settings.enviroment[\"RANDOM_STATE\"],\n",
    "                                                 n_iter=n_iter)\n",
    "            else:\n",
    "                print(f\"Создается и выполняется GridSearchCV для модели {model_name} класса {model_class.__name__}\")\n",
    "                grid_search = GridSearchCV(model_class(), param_grid, cv=5, n_jobs=n_jobs, \n",
    "                                           verbose=verbose, scoring=scoring)\n",
    "            \n",
    "            # Обучаем модель на данных с использованием кросс-валидации\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "            if need_save:\n",
    "                print(f\"\\nРезультаты поиска оптимальных гиперпараметров модели сохранены в {grid_search_filename}\")\n",
    "                _ = joblib.dump(grid_search, grid_search_filename)\n",
    "        return grid_search    \n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_metrcis(model_wraps):\n",
    "        \"\"\"Сформировать датафрейм с метриками моделей из списка model_wraps\"\"\"\n",
    "        df_metrics = []\n",
    "        for model_wrap in model_wraps:\n",
    "            df_metrics.append(pd.DataFrame(model_wrap.metrics()))\n",
    "\n",
    "        df_stat = pd.concat(df_metrics)\n",
    "        columns = ['model_name']\n",
    "        columns = columns + model_wraps[0].metrics_names()\n",
    "        df_stat2 = df_stat.pivot_table(columns = 'params',\n",
    "                                        index='model_name',\n",
    "                                        values='values').reset_index()[columns]\n",
    "        return df_stat2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelWrapRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наборы метрик для оценки моделей регрессии\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # Средняя квадратичная ошибка для регрессии\n",
    "    mean_absolute_error, \n",
    "    root_mean_squared_error, \n",
    "    r2_score  # Коэффициент детерминации для регрессии\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapRegression(ModelWrapBase):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "        self.mse_train = None\n",
    "        self.r2_train = None\n",
    "        self.rmse_train = None\n",
    "        self.mae_train = None\n",
    "\n",
    "        self.mse_test = None\n",
    "        self.r2_test = None\n",
    "        self.rmse_test = None\n",
    "        self.mae_test = None\n",
    "\n",
    "    def calc_metrics(self):\n",
    "        \"\"\"Посчитать метрики модели\"\"\"\n",
    "        self.y_train_pred = self.model.predict(self.X_train)\n",
    "        self.y_test_pred = self.model.predict(self.X_test)\n",
    "        \n",
    "        self.mse_train = mean_squared_error(self.y_train, self.y_train_pred)\n",
    "        self.r2_train = r2_score(self.y_train, self.y_train_pred)\n",
    "        self.rmse_train = root_mean_squared_error(self.y_train, self.y_train_pred)\n",
    "        self.mae_train = mean_absolute_error(self.y_train, self.y_train_pred)   \n",
    "        self.median_train = self.y_train.median() \n",
    "\n",
    "        self.mse_test = mean_squared_error(self.y_test, self.y_test_pred)\n",
    "        self.r2_test = r2_score(self.y_test, self.y_test_pred)\n",
    "        self.rmse_test = root_mean_squared_error(self.y_test, self.y_test_pred)\n",
    "        self.mae_test = mean_absolute_error(self.y_test, self.y_test_pred)    \n",
    "        self.median_test = self.y_test.median() \n",
    "    \n",
    "        \n",
    "    def show_quality(self): \n",
    "        \"\"\"Показать различные метрики\"\"\"\n",
    "        print('Train data:')\n",
    "        print(f\"  MSE:    {round(self.mse_train,4)}\")\n",
    "        print(f\"  RMSE:   {round(self.rmse_train,4)}\")\n",
    "        print(f\"  MAE:    {round(self.mae_train,4)}\")\n",
    "        print(f\"  r2:     {round(self.r2_train,4)}\")\n",
    "        print(f\"  median: {round(self.median_train,4)}\")\n",
    "\n",
    "        print('Test data:')\n",
    "        print(f\"  MSE:    {round(self.mse_test,4)}\")\n",
    "        print(f\"  RMSE:   {round(self.rmse_test,4)}\")\n",
    "        print(f\"  MAE:    {round(self.mae_test,4)}\")\n",
    "        print(f\"  r2:     {round(self.r2_test,4)}\")    \n",
    "        print(f\"  median: {round(self.median_train,4)}\")        \n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics_names():\n",
    "        return ['Train_MSE', 'Test_MSE',\n",
    "                'Train_RMSE', 'Test_RMSE',\n",
    "                'Train_MAE', 'Test_MAE',\n",
    "                'Train_R2', 'Test_R2',\n",
    "                'Train_median', 'Test_Median'\n",
    "                ]\n",
    "    \n",
    "    def metrics(self):\n",
    "        \"\"\"Сформировать словарь о сзначениями метрик модели\"\"\"\n",
    "        metrics_as_dict = {\n",
    "                'params': ModelWrapRegression.metrics_names(),\n",
    "                'values': [\n",
    "                    self.mse_train, self.mse_test,\n",
    "                    self.rmse_train, self.rmse_test,\n",
    "                    self.mae_train, self.mae_test,\n",
    "                    self.r2_train, self.r2_test,\n",
    "                    self.median_train, self.median_train\n",
    "                ],\n",
    "                'model_name': [self.name for i in range(len(ModelWrapRegression.metrics_names()))]\n",
    "            }      \n",
    "        return metrics_as_dict\n",
    "\n",
    "    @staticmethod    \n",
    "    def load_or_create_and_fit_model(model_name, model_class, model_params, \n",
    "                                    X_train, X_test, y_train, y_test,\n",
    "                                    settings, \n",
    "                                    need_save=True):\n",
    "        \"\"\"Загрузить ранее обученную модель из кеша.\n",
    "        Если в кеше нет - обучить на переданных данных с заданными параметрами.\n",
    "        \"\"\"\n",
    "        return ModelWrapBase._load_or_create_and_fit_model(ModelWrapRegression, \n",
    "                                                       model_name, model_class, model_params, \n",
    "                                                       X_train, X_test, y_train, y_test,\n",
    "                                                       settings, \n",
    "                                                       need_save)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelWrapClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наборы метрик для оценки моделей классификации\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    f1_score,  #f1-мера\n",
    "    accuracy_score,  # Метрика точности для классификации\n",
    "    classification_report,  # Отчет о классификации\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapClass(ModelWrapBase):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "        self.train_precision = None\n",
    "        self.test_precision = None\n",
    "        self.train_recall = None\n",
    "        self.test_recall = None\n",
    "        self.train_roc_auc = None\n",
    "        self.test_roc_auc = None\n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.train_f1_score = None\n",
    "        self.test_f1_score = None\n",
    "        self.specific_data = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_train_proba = None\n",
    "        self.y_test_proba = None\n",
    "\n",
    "    def calc_metrics(self):\n",
    "        \"\"\"Посчитать метрики модели\"\"\"\n",
    "        self.y_train_pred = self.model.predict(self.X_train)\n",
    "        self.y_train_prob = self.model.predict_proba(self.X_train)[:, 1]\n",
    "        self.y_test_pred = self.model.predict(self.X_test)\n",
    "        self.y_test_prob = self.model.predict_proba(self.X_test)[:, 1]\n",
    "    \n",
    "        # матрица ошибок\n",
    "        #self.conf_matrix_train = confusion_matrix(self.y_train, self.y_train_pred)\n",
    "        #self.conf_matrix_test = confusion_matrix(self.y_test, self.y_test_pred)\n",
    "        #self.conf_matrix_norm_train = confusion_matrix(self.y_train, self.y_train_pred, normalize='all')\n",
    "        #self.conf_matrix_norm_test = confusion_matrix(self.y_test, self.y_test_pred, normalize='all')\n",
    "        \n",
    "\n",
    "        # Расчет AUC-ROC\n",
    "        self.train_roc_auc = roc_auc_score(self.y_train, self.y_train_prob)\n",
    "        self.test_roc_auc = roc_auc_score(self.y_test, self.y_test_prob)\n",
    "\n",
    "        # Поиск порога, максимизирующего F1-score\n",
    "        thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "        f1_scores = [f1_score(self.y_test, self.y_test_prob >= t) for t in thresholds]\n",
    "        self.optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "        # Пересчет метрик с учетом оптимального порога\n",
    "        self.y_train_pred_optimal = (self.y_train_prob >= self.optimal_threshold).astype(int)\n",
    "        self.y_test_pred_optimal = (self.y_test_prob >= self.optimal_threshold).astype(int)\n",
    "\n",
    "        self.train_precision = precision_score(self.y_train, self.y_train_pred_optimal)\n",
    "        self.test_precision = precision_score(self.y_test, self.y_test_pred_optimal)\n",
    "\n",
    "        self.train_recall = recall_score(self.y_train, self.y_train_pred_optimal)\n",
    "        self.test_recall = recall_score(self.y_test, self.y_test_pred_optimal)\n",
    "\n",
    "        self.train_accuracy = accuracy_score(self.y_train, self.y_train_pred_optimal)\n",
    "        self.test_accuracy = accuracy_score(self.y_test, self.y_test_pred_optimal)\n",
    "\n",
    "        self.train_f1_score = f1_score(self.y_train, self.y_train_pred_optimal)\n",
    "        self.test_f1_score = f1_score(self.y_test, self.y_test_pred_optimal)\n",
    "        \n",
    "    def show_quality(self): #X_train, X_test, y_train, y_test, check_result, title, grid_search, model_cl):\n",
    "        \"\"\"Показать различные метрики и промежуточные переменные обучения\"\"\"\n",
    "        #def show_quality2(X_train, X_test, y_train, y_test, check_result, title, grid_search, model_cl):\n",
    "        fig = plotly_subplt.make_subplots(rows=2, cols=2, \n",
    "                                        subplot_titles=['ROC AUC', 'Metrics', 'Confusion Matrix Train', 'Confusion Matrix Test'],\n",
    "                                        vertical_spacing = 0.1,\n",
    "                                        row_width=[0.4, 0.6])\n",
    "        fig.update_layout(\n",
    "            title_x=0.5,\n",
    "            title_text=self.name,\n",
    "            width = 1000,\n",
    "            height = 800,\n",
    "            legend = dict(yanchor=\"bottom\", y=0.63, xanchor=\"right\", x=0.44),\n",
    "            margin = {'t':80, 'b':50, 'l':10, 'r':10}\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Построение ROC кривой\n",
    "        fpr_test, tpr_test, thresholds = roc_curve(self.y_test, self.y_test_prob)\n",
    "        fpr_train, tpr_train, thresholds = roc_curve(self.y_train, self.y_train_prob)\n",
    "        roc_train_g = plotly_go.Scatter(x=fpr_train, y=tpr_train, name=\"ROC curve Train\", line={'color':'green'})\n",
    "        roc_test_g = plotly_go.Scatter(x=fpr_test, y=tpr_test, name=\"ROC curve Test\", line={'color':'blue'})\n",
    "        roc_diag_g = plotly_go.Scatter(x=[0, 1], y=[0, 1], line={'color':'gray', 'dash': 'dash'}, showlegend=False)\n",
    "\n",
    "        fig.add_trace(roc_train_g, row=1, col=1)\n",
    "        fig.add_trace(roc_test_g, row=1, col=1)\n",
    "        fig.add_trace(roc_diag_g, row=1, col=1)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis1 = {'title_text': \"False Positive Rate\"},\n",
    "            yaxis1 = {'title_text': \"True Positive Rate\"}\n",
    "        )    \n",
    "        \n",
    "\n",
    "        # Bar с метриками\n",
    "        df_metrics = pd.DataFrame([[self.test_accuracy,  self.train_accuracy],\n",
    "                                   [self.test_precision, self.train_precision],\n",
    "                                   [self.test_recall,    self.train_recall],\n",
    "                                   [self.test_roc_auc,   self.train_roc_auc],\n",
    "                                   [self.test_f1_score,  self.train_f1_score]], \n",
    "                                  columns = [\"Test\", \"Train\"], \n",
    "                                  index=[\"accuracy\", \"precision\", \"recall\", \"ROC AUC\", \"F1\"])\n",
    "        metrics_train = plotly_go.Bar(x=df_metrics.index, y=df_metrics.Train, \n",
    "                        showlegend=True, text=round(df_metrics.Train,4), textangle=0, \n",
    "                        xaxis='x2', yaxis='y2', name=\"Train Metrics\")\n",
    "        metrics_test = plotly_go.Bar(x=df_metrics.index, y=df_metrics.Test, \n",
    "                        showlegend=True, text=round(df_metrics.Test,4), textangle=0, \n",
    "                        xaxis='x2', yaxis='y2', name=\"Test Metrics\")\n",
    "\n",
    "        fig.add_trace(metrics_train, row=1, col=2) \n",
    "        fig.add_trace(metrics_test, row=1, col=2) \n",
    "\n",
    "        # Confusion Matrix \n",
    "        \"\"\"\n",
    "        cm_normalized_train = self.conf_matrix_train.astype('float') / self.conf_matrix_train.sum(axis=1)[:, np.newaxis]\n",
    "        print(self.conf_matrix_train.astype('float'))\n",
    "        print(self.conf_matrix_train.sum(axis=1).astype('float'))\n",
    "        print(cm_normalized_train)\n",
    "        heatmap_train = plotly_go.Heatmap(z=cm_normalized_train, x=['0', '1'], y=['0', '1'], colorscale='Blues', \n",
    "                                        text=np.round(cm_normalized_train, 3), texttemplate=\"%{text}\", showscale=False)\n",
    "\n",
    "        cm_normalized_test = self.conf_matrix_test.astype('float') / self.conf_matrix_test.sum(axis=1)[:, np.newaxis]\n",
    "        heatmap_test = plotly_go.Heatmap(z=cm_normalized_test, x=['0', '1'], y=['0', '1'], colorscale='Blues', \n",
    "                                        text=np.round(cm_normalized_test, 3), texttemplate=\"%{text}\", showscale=False)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        train_cm = confusion_matrix(self.y_train, self.y_train_pred_optimal, normalize='all')\n",
    "        heatmap_train = plotly_go.Heatmap(z=train_cm, \n",
    "                                          x=['0', '1'], y=['0', '1'], \n",
    "                                          colorscale='Blues', \n",
    "                                          text=np.round(train_cm, 3), \n",
    "                                          texttemplate=\"%{text}\", \n",
    "                                          showscale=False)\n",
    "\n",
    "        test_cm = confusion_matrix(self.y_test, self.y_test_pred_optimal, normalize='all')\n",
    "        heatmap_test = plotly_go.Heatmap(z=test_cm, \n",
    "                                         x=['0', '1'], y=['0', '1'], \n",
    "                                         colorscale='Blues', \n",
    "                                         text=np.round(test_cm, 3), \n",
    "                                         texttemplate=\"%{text}\", \n",
    "                                         showscale=False)\n",
    "\n",
    "\n",
    "        fig.add_trace(heatmap_train, row=2, col=1)\n",
    "        fig.add_trace(heatmap_test,  row=2, col=2) \n",
    "\n",
    "        fig.update_layout(\n",
    "            xaxis1 = {'title': 'Predict'},\n",
    "            xaxis2 = {'title': 'Predict'},\n",
    "            yaxis1 = {'title': 'Goals'},\n",
    "            yaxis2 = {'title': 'Goals'},\n",
    "            xaxis3 = {'title': 'Предсказания'},\n",
    "            xaxis4 = {'title': 'Предсказания'},\n",
    "            yaxis3 = {'title': 'Факт'},\n",
    "            yaxis4 = {'title': 'Факт'},\n",
    "                        \n",
    "        )    \n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics_names():\n",
    "        return ['Training_Precision', 'Test_Precision',\n",
    "                'Training_Recall', 'Test_Recall',\n",
    "                'ROC_AUC_Train', 'ROC_AUC_Test',\n",
    "                'Accuarcy_Train', 'Accuarcy_Test',\n",
    "                'F1_score_Train', 'F1_score_Test'\n",
    "                ]\n",
    "    \n",
    "    def metrics(self):\n",
    "        \"\"\"Сформировать словарь о сзначениями метрик модели\"\"\"\n",
    "        metrics_as_dict = {\n",
    "                'params': ModelWrapClass.metrics_names(),\n",
    "                'values': [\n",
    "                    self.train_precision, self.test_precision,\n",
    "                    self.train_recall, self.test_recall,\n",
    "                    self.train_roc_auc, self.test_roc_auc,\n",
    "                    self.train_accuracy, self.test_accuracy,\n",
    "                    self.train_f1_score, self.test_f1_score\n",
    "                ],\n",
    "                'model_name': [self.name for i in range(len(ModelWrapClass.metrics_names()))]\n",
    "            }      \n",
    "        return metrics_as_dict\n",
    "\n",
    "    @staticmethod    \n",
    "    def load_or_create_and_fit_model(model_name, model_class, model_params, \n",
    "                                    X_train, X_test, y_train, y_test,\n",
    "                                    settings, \n",
    "                                    need_save=True):\n",
    "        \"\"\"Загрузить ранее обученную модель из кеша.\n",
    "        Если в кеше нет - обучить на переданных данных с заданными параметрами.\n",
    "        \"\"\"\n",
    "        return ModelWrapBase._load_or_create_and_fit_model(ModelWrapClass, \n",
    "                                                       model_name, model_class, model_params, \n",
    "                                                       X_train, X_test, y_train, y_test,\n",
    "                                                       settings, \n",
    "                                                       need_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapRegression(ModelWrapBase):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "        self.mse_train = None\n",
    "        self.r2_train = None\n",
    "        self.rmse_train = None\n",
    "        self.mae_train = None\n",
    "\n",
    "        self.mse_test = None\n",
    "        self.r2_test = None\n",
    "        self.rmse_test = None\n",
    "        self.mae_test = None\n",
    "\n",
    "    def calc_metrics(self):\n",
    "        \"\"\"Посчитать метрики модели\"\"\"\n",
    "        self.y_train_pred = self.model.predict(self.X_train)\n",
    "        self.y_test_pred = self.model.predict(self.X_test)\n",
    "        \n",
    "        self.mse_train = mean_squared_error(self.y_train, self.y_train_pred)\n",
    "        self.r2_train = r2_score(self.y_train, self.y_train_pred)\n",
    "        self.rmse_train = root_mean_squared_error(self.y_train, self.y_train_pred)\n",
    "        self.mae_train = mean_absolute_error(self.y_train, self.y_train_pred)   \n",
    "        self.median_train = self.y_train.median() \n",
    "\n",
    "        self.mse_test = mean_squared_error(self.y_test, self.y_test_pred)\n",
    "        self.r2_test = r2_score(self.y_test, self.y_test_pred)\n",
    "        self.rmse_test = root_mean_squared_error(self.y_test, self.y_test_pred)\n",
    "        self.mae_test = mean_absolute_error(self.y_test, self.y_test_pred)    \n",
    "        self.median_test = self.y_test.median() \n",
    "    \n",
    "        \n",
    "    def show_quality(self): \n",
    "        \"\"\"Показать различные метрики\"\"\"\n",
    "        print('Train data:')\n",
    "        print(f\"  MSE:    {round(self.mse_train,4)}\")\n",
    "        print(f\"  RMSE:   {round(self.rmse_train,4)}\")\n",
    "        print(f\"  MAE:    {round(self.mae_train,4)}\")\n",
    "        print(f\"  r2:     {round(self.r2_train,4)}\")\n",
    "        print(f\"  median: {round(self.median_train,4)}\")\n",
    "\n",
    "        print('Test data:')\n",
    "        print(f\"  MSE:    {round(self.mse_test,4)}\")\n",
    "        print(f\"  RMSE:   {round(self.rmse_test,4)}\")\n",
    "        print(f\"  MAE:    {round(self.mae_test,4)}\")\n",
    "        print(f\"  r2:     {round(self.r2_test,4)}\")    \n",
    "        print(f\"  median: {round(self.median_train,4)}\")        \n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics_names():\n",
    "        return ['Train_MSE', 'Test_MSE',\n",
    "                'Train_RMSE', 'Test_RMSE',\n",
    "                'Train_MAE', 'Test_MAE',\n",
    "                'Train_R2', 'Test_R2',\n",
    "                'Train_median', 'Test_Median'\n",
    "                ]\n",
    "    \n",
    "    def metrics(self):\n",
    "        \"\"\"Сформировать словарь о сзначениями метрик модели\"\"\"\n",
    "        metrics_as_dict = {\n",
    "                'params': ModelWrapRegression.metrics_names(),\n",
    "                'values': [\n",
    "                    self.mse_train, self.mse_test,\n",
    "                    self.rmse_train, self.rmse_test,\n",
    "                    self.mae_train, self.mae_test,\n",
    "                    self.r2_train, self.r2_test,\n",
    "                    self.median_train, self.median_train\n",
    "                ],\n",
    "                'model_name': [self.name for i in range(len(ModelWrapRegression.metrics_names()))]\n",
    "            }      \n",
    "        return metrics_as_dict\n",
    "\n",
    "    @staticmethod    \n",
    "    def load_or_create_and_fit_model(model_name, model_class, model_params, \n",
    "                                    X_train, X_test, y_train, y_test,\n",
    "                                    settings, \n",
    "                                    need_save=True):\n",
    "        \"\"\"Загрузить ранее обученную модель из кеша.\n",
    "        Если в кеше нет - обучить на переданных данных с заданными параметрами.\n",
    "        \"\"\"\n",
    "        return ModelWrapBase._load_or_create_and_fit_model(ModelWrapRegression, \n",
    "                                                       model_name, model_class, model_params, \n",
    "                                                       X_train, X_test, y_train, y_test,\n",
    "                                                       settings, \n",
    "                                                       need_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурирование среды и окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 50) # Устанавливаем максимальное количество отображаемых столбцов равным 50\n",
    "#pd.set_option('display.max_rows', 50) # Устанавливаем максимальное количество отображаемых строк равным 20\n",
    "pd.options.display.float_format = '{:.5f}'.format # Устанавливаем формат отображения чисел с двумя знаками после запятой\n",
    "pd.options.mode.use_inf_as_na = True # Настройка режима Pandas для рассмотрения бесконечностей (inf) как пропущенных значений (NA)\n",
    "\n",
    "# Конфигурация формата отображения графиков в виде векторных изображений\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# для построения графиков внутри Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_filename = \"settings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(settings_filename).exists():\n",
    "    with open(settings_filename, \"w\") as f:\n",
    "        f.write(\"\"\"# Каталог с датасетом\n",
    "DATASET_SUBFOLDER=dataset\n",
    "# Каталог для результатов и промежуточных файлов\n",
    "RESULT_SUBFOLDER=result\n",
    "# Каталог для кеша промежуточных результатов\n",
    "CACHE_SUBFOLDER=cached_results\n",
    "# Каталог для boxplot\n",
    "BOXPLOT_SUBFOLDER=boxplot\n",
    "\n",
    "RANDOM_STATE=42\n",
    "\n",
    "DATASET_FILENAME_TEMPLATE=dataset_df_%s.joblib\n",
    "PARAMS_FILENAME_TEMPLATE=params_%s.joblib\n",
    "\n",
    "X_Train_FILENAME_TEMPLATE=X_Train_%s.joblib\n",
    "y_Train_FILENAME_TEMPLATE=y_Train_%s.joblib\n",
    "X_Test_FILENAME_TEMPLATE=X_Test_%s.joblib\n",
    "y_Test_FILENAME_TEMPLATE=y_Test_%s.joblib\n",
    "\n",
    "\n",
    "# Шаблоны для имен\n",
    "GRID_SEARCH_TEMPLATE_FILENAME=03_GridSearch_%s.joblib\n",
    "MODEL_CLASS_TEMPLATE_FILENAME=04_model_%s.joblib\"\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить параметры\n",
    "settings_dict = {\n",
    "    **dotenv_values(settings_filename)\n",
    "}\n",
    "\n",
    "settings = Settings(settings_dict)\n",
    "settings.enviroment[\"RANDOM_STATE\"] = int(settings.enviroment[\"RANDOM_STATE\"])\n",
    "n_jobs = -1\n",
    "verbose = 2\n",
    "load_from_kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATASET_SUBFOLDER': 'dataset',\n",
       " 'RESULT_SUBFOLDER': 'result',\n",
       " 'CACHE_SUBFOLDER': 'cached_results',\n",
       " 'BOXPLOT_SUBFOLDER': 'boxplot',\n",
       " 'RANDOM_STATE': 42,\n",
       " 'DATASET_FILENAME_TEMPLATE': 'dataset_df_%s.joblib',\n",
       " 'PARAMS_FILENAME_TEMPLATE': 'params_%s.joblib',\n",
       " 'X_Train_FILENAME_TEMPLATE': 'X_Train_%s.joblib',\n",
       " 'y_Train_FILENAME_TEMPLATE': 'y_Train_%s.joblib',\n",
       " 'X_Test_FILENAME_TEMPLATE': 'X_Test_%s.joblib',\n",
       " 'y_Test_FILENAME_TEMPLATE': 'y_Test_%s.joblib',\n",
       " 'GRID_SEARCH_TEMPLATE_FILENAME': '03_GridSearch_%s.joblib',\n",
       " 'MODEL_CLASS_TEMPLATE_FILENAME': '04_model_%s.joblib'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель H2O AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML # загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>1 hour 59 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Samara</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>16 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_igel_yhbs9q</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>15.60 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.9 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         1 hour 59 mins\n",
       "H2O_cluster_timezone:       Europe/Samara\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.7\n",
       "H2O_cluster_version_age:    16 days\n",
       "H2O_cluster_name:           H2O_from_python_igel_yhbs9q\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    15.60 Gb\n",
       "H2O_cluster_total_cores:    20\n",
       "H2O_cluster_allowed_cores:  20\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.9 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o_model_name = \"h2o\"\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить датасет если его нет\n",
    "ts_csv_filename = Path(settings.enviroment[\"DATASET_SUBFOLDER\"], \"\", 'Electric_Production.csv')\n",
    "if not Path(ts_csv_filename).exists():\n",
    "    if not Path(settings.enviroment[\"DATASET_SUBFOLDER\"]).exists():\n",
    "        Path.mkdir(Path(settings.enviroment[\"DATASET_SUBFOLDER\"]))\n",
    "    od.download_url(\"https://raw.githubusercontent.com/ejgao/Time-Series-Datasets/refs/heads/master/Electric_Production.csv\", \n",
    "                                Path(settings.enviroment[\"DATASET_SUBFOLDER\"]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>IPG2211A2N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/1985</td>\n",
       "      <td>72.50520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/1/1985</td>\n",
       "      <td>70.67200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/1/1985</td>\n",
       "      <td>62.45020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/1/1985</td>\n",
       "      <td>57.47140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/1/1985</td>\n",
       "      <td>55.31510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>9/1/2017</td>\n",
       "      <td>98.61540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>10/1/2017</td>\n",
       "      <td>93.61370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>11/1/2017</td>\n",
       "      <td>97.33590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>12/1/2017</td>\n",
       "      <td>114.72120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1/1/2018</td>\n",
       "      <td>129.40480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATE  IPG2211A2N\n",
       "0     1/1/1985    72.50520\n",
       "1     2/1/1985    70.67200\n",
       "2     3/1/1985    62.45020\n",
       "3     4/1/1985    57.47140\n",
       "4     5/1/1985    55.31510\n",
       "..         ...         ...\n",
       "392   9/1/2017    98.61540\n",
       "393  10/1/2017    93.61370\n",
       "394  11/1/2017    97.33590\n",
       "395  12/1/2017   114.72120\n",
       "396   1/1/2018   129.40480\n",
       "\n",
       "[397 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим датасет и для ДЗ отберем 5000 строк из датасета\n",
    "original_dataset_df = pd.read_csv(ts_csv_filename)\n",
    "dataset_df = original_dataset_df.copy()\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 397 entries, 0 to 396\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   DATE        397 non-null    object \n",
      " 1   IPG2211A2N  397 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Преобразование даты в индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дату в индекс\n",
    "dataset_df['Date'] = pd.to_datetime(dataset_df['DATE']) # Строки преобразуем в даты\n",
    "dataset_df.set_index('Date', inplace=True)\n",
    "dataset_df.drop('DATE', inplace=True, axis=1)\n",
    "dataset_df.columns = ['EP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим два последних года в тестовую выборку\n",
    "len_test = 24\n",
    "train = dataset_df.head(len(dataset_df)-len_test)\n",
    "test =  dataset_df.tail(len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>2 hours 13 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Samara</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>16 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_igel_yhbs9q</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>15.60 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.9 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         2 hours 13 mins\n",
       "H2O_cluster_timezone:       Europe/Samara\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.7\n",
       "H2O_cluster_version_age:    16 days\n",
       "H2O_cluster_name:           H2O_from_python_igel_yhbs9q\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    15.60 Gb\n",
       "H2O_cluster_total_cores:    20\n",
       "H2O_cluster_allowed_cores:  20\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.9 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "# Инициализация H2O\n",
    "h2o.init()\n",
    "\n",
    "# Конвертация в H2O Frame\n",
    "train_h2o = h2o.H2OFrame(train)\n",
    "test_h2o = h2o.H2OFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class='dataframe'>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">     EP</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">72.5052</td></tr>\n",
       "<tr><td style=\"text-align: right;\">70.672 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">62.4502</td></tr>\n",
       "<tr><td style=\"text-align: right;\">57.4714</td></tr>\n",
       "<tr><td style=\"text-align: right;\">55.3151</td></tr>\n",
       "<tr><td style=\"text-align: right;\">58.0904</td></tr>\n",
       "<tr><td style=\"text-align: right;\">62.6202</td></tr>\n",
       "<tr><td style=\"text-align: right;\">63.2485</td></tr>\n",
       "<tr><td style=\"text-align: right;\">60.5846</td></tr>\n",
       "<tr><td style=\"text-align: right;\">56.3154</td></tr>\n",
       "</tbody>\n",
       "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[373 rows x 1 column]</pre>"
      ],
      "text/plain": [
       "     EP\n",
       "-------\n",
       "72.5052\n",
       "70.672\n",
       "62.4502\n",
       "57.4714\n",
       "55.3151\n",
       "58.0904\n",
       "62.6202\n",
       "63.2485\n",
       "60.5846\n",
       "56.3154\n",
       "[373 rows x 1 column]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка AutoML\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=3600,  # Максимальное время обучения\n",
    "    exclude_algos=[\"DeepLearning\"],  # Исключить алгоритмы при необходимости\n",
    "    seed=42,\n",
    "    nfolds=0  # Для временных рядов кросс-валидация не рекомендуется!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "aml.train(x=x, y=y, training_frame=train_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-01-01</th>\n",
       "      <td>72.50520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-02-01</th>\n",
       "      <td>70.67200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-03-01</th>\n",
       "      <td>62.45020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-04-01</th>\n",
       "      <td>57.47140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-01</th>\n",
       "      <td>55.31510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>98.61540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>93.61370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>97.33590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>114.72120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>129.40480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  EP\n",
       "Date                \n",
       "1985-01-01  72.50520\n",
       "1985-02-01  70.67200\n",
       "1985-03-01  62.45020\n",
       "1985-04-01  57.47140\n",
       "1985-05-01  55.31510\n",
       "...              ...\n",
       "2017-09-01  98.61540\n",
       "2017-10-01  93.61370\n",
       "2017-11-01  97.33590\n",
       "2017-12-01 114.72120\n",
       "2018-01-01 129.40480\n",
       "\n",
       "[397 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "H2OServerError",
     "evalue": "HTTP 500 Server Error:\nServer error water.util.DistributedException:\n  Error: DistributedException from /127.0.0.1:54321: 'This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.'\n  Request: None\n  Stacktrace: DistributedException from /127.0.0.1:54321: 'This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.', caused by java.lang.RuntimeException: This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.\n      water.MRTask.getResult(MRTask.java:660)\n      water.MRTask.getResult(MRTask.java:670)\n      water.MRTask.doAll(MRTask.java:555)\n      water.parser.ParseSetup.guessSetup(ParseSetup.java:408)\n      water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:44)\n      java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n      java.base/java.lang.reflect.Method.invoke(Method.java:580)\n      water.api.Handler.handle(Handler.java:60)\n      water.api.RequestServer.serve(RequestServer.java:472)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OServerError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mimport_file(nf, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/h2o.py:503\u001b[0m, in \u001b[0;36mimport_file\u001b[0;34m(path, destination_frame, parse, header, sep, col_names, col_types, na_strings, pattern, skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lazy_import(path, pattern)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m H2OFrame()\u001b[38;5;241m.\u001b[39m_import_parse(path, pattern, destination_frame, header, sep, col_names, col_types, na_strings,\n\u001b[1;32m    504\u001b[0m                                     skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/frame.py:459\u001b[0m, in \u001b[0;36mH2OFrame._import_parse\u001b[0;34m(self, path, pattern, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\u001b[0m\n\u001b[1;32m    457\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(path)\n\u001b[1;32m    458\u001b[0m rawkey \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mlazy_import(path, pattern)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(rawkey, destination_frame, header, separator, column_names, column_types, na_strings,\n\u001b[1;32m    460\u001b[0m             skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, \n\u001b[1;32m    461\u001b[0m             escapechar, tz_adjust_to_local)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/frame.py:475\u001b[0m, in \u001b[0;36mH2OFrame._parse\u001b[0;34m(self, rawkey, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parse\u001b[39m(\u001b[38;5;28mself\u001b[39m, rawkey, destination_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, column_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    473\u001b[0m            na_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skipped_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_col_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, custom_non_data_line_markers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partition_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    474\u001b[0m            escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tz_adjust_to_local\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 475\u001b[0m     setup \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mparse_setup(rawkey, destination_frame, header, separator, column_names, column_types, na_strings,\n\u001b[1;32m    476\u001b[0m                             skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_raw(setup)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/h2o.py:858\u001b[0m, in \u001b[0;36mparse_setup\u001b[0;34m(raw_frames, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, force_col_types, custom_non_data_line_markers, partition_by, quotechar, escapechar, tz_adjust_to_local)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition_by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition_by\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m partition_by\n\u001b[0;32m--> 858\u001b[0m j \u001b[38;5;241m=\u001b[39m api(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST /3/ParseSetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarnings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m j \u001b[38;5;129;01mand\u001b[39;00m j[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarnings\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m j[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarnings\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/h2o.py:123\u001b[0m, in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# type checks are performed in H2OConnection class\u001b[39;00m\n\u001b[1;32m    122\u001b[0m _check_connection()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h2oconn\u001b[38;5;241m.\u001b[39mrequest(endpoint, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, filename\u001b[38;5;241m=\u001b[39mfilename, save_to\u001b[38;5;241m=\u001b[39msave_to)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/backend/connection.py:499\u001b[0m, in \u001b[0;36mH2OConnection.request\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    497\u001b[0m         save_to \u001b[38;5;241m=\u001b[39m save_to(resp)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_end_transaction(start_time, resp)\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(resp, save_to)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_server \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_server\u001b[38;5;241m.\u001b[39mis_running():\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/h2o/backend/connection.py:858\u001b[0m, in \u001b[0;36mH2OConnection._process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m H2OResponseError(data)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# Server errors (notably 500 = \"Server Error\")\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# did not provide the correct status code.\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m H2OServerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTTP \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (status_code, response\u001b[38;5;241m.\u001b[39mreason, data))\n",
      "\u001b[0;31mH2OServerError\u001b[0m: HTTP 500 Server Error:\nServer error water.util.DistributedException:\n  Error: DistributedException from /127.0.0.1:54321: 'This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.'\n  Request: None\n  Stacktrace: DistributedException from /127.0.0.1:54321: 'This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.', caused by java.lang.RuntimeException: This H2O node couldn't read data from 'nfs://home/igel/Projects/ml/ml-inno-hw/3. Machine Learning/3.17. AutoML 0;3>@8B<K 4;O @01>BK A 40==K<8/dataset/Electric_Production.csv'. Please make sure the file is available on all H2O nodes and/or check the working directories.\n      water.MRTask.getResult(MRTask.java:660)\n      water.MRTask.getResult(MRTask.java:670)\n      water.MRTask.doAll(MRTask.java:555)\n      water.parser.ParseSetup.guessSetup(ParseSetup.java:408)\n      water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:44)\n      java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n      java.base/java.lang.reflect.Method.invoke(Method.java:580)\n      water.api.Handler.handle(Handler.java:60)\n      water.api.RequestServer.serve(RequestServer.java:472)\n"
     ]
    }
   ],
   "source": [
    "df = h2o.import_file(nf, header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_ml = H2OAutoML(max_models = 10, seed = 1)\n",
    "h2o_ml.train(x = params[\"columns_X\"], y = params[\"target_column\"], training_frame = dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2.40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>3.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>5.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>997.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>1017.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>1037.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>1058.40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>1079.10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      value\n",
       "0  2020-01-01    0.00000\n",
       "1  2020-01-02    1.10000\n",
       "2  2020-01-03    2.40000\n",
       "3  2020-01-04    3.90000\n",
       "4  2020-01-05    5.60000\n",
       "..        ...        ...\n",
       "95 2020-04-05  997.50000\n",
       "96 2020-04-06 1017.60000\n",
       "97 2020-04-07 1037.90000\n",
       "98 2020-04-08 1058.40000\n",
       "99 2020-04-09 1079.10000\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'date': pd.date_range(start='2020-01-01', periods=100, freq='D'),\n",
    "    'value': [i + 0.1 * i**2 for i in range(100)]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values('date', inplace=True)  # Обязательно отсортируйте по времени!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>rolling_mean_7</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>11.90000</td>\n",
       "      <td>9.60000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>14.40000</td>\n",
       "      <td>11.90000</td>\n",
       "      <td>1.10000</td>\n",
       "      <td>7.90000</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>17.10000</td>\n",
       "      <td>14.40000</td>\n",
       "      <td>2.40000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>17.10000</td>\n",
       "      <td>3.90000</td>\n",
       "      <td>12.30000</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>23.10000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>5.60000</td>\n",
       "      <td>14.80000</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>997.50000</td>\n",
       "      <td>977.60000</td>\n",
       "      <td>862.40000</td>\n",
       "      <td>938.80000</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>1017.60000</td>\n",
       "      <td>997.50000</td>\n",
       "      <td>881.10000</td>\n",
       "      <td>958.30000</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>1037.90000</td>\n",
       "      <td>1017.60000</td>\n",
       "      <td>900.00000</td>\n",
       "      <td>978.00000</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>1058.40000</td>\n",
       "      <td>1037.90000</td>\n",
       "      <td>919.10000</td>\n",
       "      <td>997.90000</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>1079.10000</td>\n",
       "      <td>1058.40000</td>\n",
       "      <td>938.40000</td>\n",
       "      <td>1018.00000</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      value      lag_1     lag_7  rolling_mean_7  year  month  \\\n",
       "7  2020-01-08   11.90000    9.60000   0.00000         6.00000  2020      1   \n",
       "8  2020-01-09   14.40000   11.90000   1.10000         7.90000  2020      1   \n",
       "9  2020-01-10   17.10000   14.40000   2.40000        10.00000  2020      1   \n",
       "10 2020-01-11   20.00000   17.10000   3.90000        12.30000  2020      1   \n",
       "11 2020-01-12   23.10000   20.00000   5.60000        14.80000  2020      1   \n",
       "..        ...        ...        ...       ...             ...   ...    ...   \n",
       "95 2020-04-05  997.50000  977.60000 862.40000       938.80000  2020      4   \n",
       "96 2020-04-06 1017.60000  997.50000 881.10000       958.30000  2020      4   \n",
       "97 2020-04-07 1037.90000 1017.60000 900.00000       978.00000  2020      4   \n",
       "98 2020-04-08 1058.40000 1037.90000 919.10000       997.90000  2020      4   \n",
       "99 2020-04-09 1079.10000 1058.40000 938.40000      1018.00000  2020      4   \n",
       "\n",
       "    day  \n",
       "7     8  \n",
       "8     9  \n",
       "9    10  \n",
       "10   11  \n",
       "11   12  \n",
       "..  ...  \n",
       "95    5  \n",
       "96    6  \n",
       "97    7  \n",
       "98    8  \n",
       "99    9  \n",
       "\n",
       "[93 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лаги (значения за предыдущие периоды)\n",
    "df['lag_1'] = df['value'].shift(1)\n",
    "df['lag_7'] = df['value'].shift(7)\n",
    "\n",
    "# Скользящее среднее\n",
    "df['rolling_mean_7'] = df['value'].rolling(window=7).mean()\n",
    "\n",
    "# Временные признаки\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# Удалите строки с NaN (после создания лагов)\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['date'] < '2020-03-01']\n",
    "test = df[df['date'] >= '2020-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-3.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-3 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-3 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-3 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table th,\n",
       "#h2o-table-3 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>2 hours 16 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Samara</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>16 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_igel_yhbs9q</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>15.60 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.9 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         2 hours 16 mins\n",
       "H2O_cluster_timezone:       Europe/Samara\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.7\n",
       "H2O_cluster_version_age:    16 days\n",
       "H2O_cluster_name:           H2O_from_python_igel_yhbs9q\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    15.60 Gb\n",
       "H2O_cluster_total_cores:    20\n",
       "H2O_cluster_allowed_cores:  20\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.9 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "03:11:09.481: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "\n",
      "03:11:09.962: _train param, Dropping bad and constant columns: [year]\n",
      "03:11:10.31: _train param, Dropping bad and constant columns: [year]\n",
      "03:11:10.31: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 46.0.\n",
      "03:11:10.32: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "\n",
      "03:11:10.147: _train param, Dropping bad and constant columns: [year]\n",
      "03:11:10.296: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "\n",
      "03:11:10.425: _train param, Dropping bad and constant columns: [year]\n",
      "03:11:10.543: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "█\n",
      "03:11:10.640: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "\n",
      "03:11:10.719: _train param, Dropping bad and constant columns: [year]\n",
      "03:11:10.800: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "██████████████████████████████████████████████████████\n",
      "03:11:58.132: _train param, Dropping bad and constant columns: [year]\n",
      "\n",
      "███████ (cancelled) 100%\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# Инициализация H2O\n",
    "h2o.init()\n",
    "\n",
    "# Конвертация в H2O Frame\n",
    "train_h2o = h2o.H2OFrame(train)\n",
    "test_h2o = h2o.H2OFrame(test)\n",
    "\n",
    "# Определение признаков и целевой переменной\n",
    "x = ['lag_1', 'lag_7', 'rolling_mean_7', 'year', 'month', 'day']\n",
    "y = 'value'\n",
    "\n",
    "# Настройка AutoML\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=3600,  # Максимальное время обучения\n",
    "    exclude_algos=[\"DeepLearning\"],  # Исключить алгоритмы при необходимости\n",
    "    seed=42,\n",
    "    nfolds=0  # Для временных рядов кросс-валидация не рекомендуется!\n",
    ")\n",
    "\n",
    "# Запуск обучения\n",
    "aml.train(x=x, y=y, training_frame=train_h2o)\n",
    "\n",
    "# Просмотр результатов\n",
    "lb = aml.leaderboard\n",
    "print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прогноз на тестовых данных\n",
    "preds = aml.leader.predict(test_h2o)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Конвертация предсказаний обратно в pandas\n",
    "test_pred = preds.as_data_frame()['predict']\n",
    "true_values = test['value'].values\n",
    "\n",
    "mae = mean_absolute_error(true_values, test_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = InputData.from_dataframe(dataset_df_wo_outliers_minmax[params[\"columns_X\"]],\n",
    "                                dataset_df_wo_outliers_minmax[params[\"target_column\"]],\n",
    "                                task='classification')\n",
    "fedot_train, fedot_test = train_test_data_setup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fedot_model = Fedot(problem='classification', metric=['accuracy', 'roc_auc', 'precision', 'f1'], timeout=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BaseLib.st()\n",
    "best_pipeline = fedot_model.fit(features=fedot_train, target='target')\n",
    "fedot_durarion_fit = BaseLib.ft(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Модель: {best_pipeline.primary_nodes}')\n",
    "print(f'Параметры модели: {pformat(best_pipeline.primary_nodes[0].parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт метрик для оценки качества моделей классификации\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    f1_score,  #f1-мера\n",
    "    accuracy_score,  # Метрика точности для классификации\n",
    "    classification_report,  # Отчет о классификации\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "\"\"\"Посчитать метрики модели\"\"\"\n",
    "f_y_train_pred = fedot_model.predict(features=fedot_train)\n",
    "f_y_train_prob = fedot_model.predict_proba(fedot_train) #[:, 1]\n",
    "f_y_test_pred = fedot_model.predict(features=fedot_test)\n",
    "f_y_test_prob = fedot_model.predict_proba(fedot_test) #[:, 1]\n",
    "\n",
    "# Расчет AUC-ROC\n",
    "f_train_roc_auc = roc_auc_score(fedot_train.target, f_y_train_prob)\n",
    "f_test_roc_auc = roc_auc_score(fedot_test.target, f_y_test_prob)\n",
    "                             \n",
    "# Поиск порога, максимизирующего F1-score\n",
    "f_thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "f_f1_scores = [f1_score(fedot_test.target, f_y_test_prob >= t) for t in f_thresholds]\n",
    "f_optimal_threshold = f_thresholds[np.argmax(f_f1_scores)]\n",
    "\n",
    "# Пересчет метрик с учетом оптимального порога\n",
    "f_y_train_pred_optimal = (f_y_train_prob >= f_optimal_threshold).astype(int)\n",
    "f_y_test_pred_optimal = (f_y_test_prob >= f_optimal_threshold).astype(int)\n",
    "\n",
    "f_train_precision = precision_score(fedot_train.target, f_y_train_pred_optimal)\n",
    "f_test_precision = precision_score(fedot_test.target, f_y_test_pred_optimal)\n",
    "\n",
    "f_train_recall = recall_score(fedot_train.target, f_y_train_pred_optimal)\n",
    "f_test_recall = recall_score(fedot_test.target, f_y_test_pred_optimal)\n",
    "\n",
    "f_train_accuracy = accuracy_score(fedot_train.target, f_y_train_pred_optimal)\n",
    "f_test_accuracy = accuracy_score(fedot_test.target, f_y_test_pred_optimal)\n",
    "\n",
    "f_train_f1_score = f1_score(fedot_train.target, f_y_train_pred_optimal)\n",
    "f_test_f1_score = f1_score(fedot_test.target, f_y_test_pred_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as plotly_px\n",
    "import plotly.graph_objects as plotly_go\n",
    "import plotly.subplots as plotly_subplt\n",
    "\n",
    "f_fig = plotly_subplt.make_subplots(rows=2, cols=2, \n",
    "                                subplot_titles=['ROC AUC', 'Metrics', 'Confusion Matrix Train', 'Confusion Matrix Test'],\n",
    "                                vertical_spacing = 0.1,\n",
    "                                row_width=[0.4, 0.6])\n",
    "f_fig.update_layout(\n",
    "    title_x=0.5,\n",
    "    title_text=\"FEDOT\",\n",
    "    width = 1000,\n",
    "    height = 800,\n",
    "    legend = dict(yanchor=\"bottom\", y=0.63, xanchor=\"right\", x=0.44),\n",
    "    margin = {'t':80, 'b':50, 'l':10, 'r':10}\n",
    "    \n",
    ")\n",
    "\n",
    "# Построение ROC кривой\n",
    "fpr_test, tpr_test, f_thresholds = roc_curve(fedot_test.target, fedot_model.predict_proba(features=fedot_test))\n",
    "fpr_train, tpr_train, f_thresholds = roc_curve(fedot_train.target, fedot_model.predict_proba(features=fedot_train))\n",
    "roc_train_g = plotly_go.Scatter(x=fpr_train, y=tpr_train, name=\"ROC curve Train\", line={'color':'green'})\n",
    "roc_test_g = plotly_go.Scatter(x=fpr_test, y=tpr_test, name=\"ROC curve Test\", line={'color':'blue'})\n",
    "roc_diag_g = plotly_go.Scatter(x=[0, 1], y=[0, 1], line={'color':'gray', 'dash': 'dash'}, showlegend=False)\n",
    "\n",
    "f_fig.add_trace(roc_train_g, row=1, col=1)\n",
    "f_fig.add_trace(roc_test_g, row=1, col=1)\n",
    "f_fig.add_trace(roc_diag_g, row=1, col=1)\n",
    "\n",
    "f_fig.update_layout(\n",
    "    xaxis1 = {'title_text': \"False Positive Rate\"},\n",
    "    yaxis1 = {'title_text': \"True Positive Rate\"}\n",
    ")    \n",
    "\n",
    "# Bar с метриками\n",
    "df_metrics = pd.DataFrame([[f_test_accuracy,  f_train_accuracy],\n",
    "                            [f_test_precision, f_train_precision],\n",
    "                            [f_test_recall,    f_train_recall],\n",
    "                            [f_test_roc_auc,   f_train_roc_auc],\n",
    "                            [f_test_f1_score,  f_train_f1_score]], \n",
    "                            columns = [\"Test\", \"Train\"], \n",
    "                            index=[\"accuracy\", \"precision\", \"recall\", \"ROC AUC\", \"F1\"])\n",
    "metrics_train = plotly_go.Bar(x=df_metrics.index, y=df_metrics.Train, \n",
    "                showlegend=True, text=round(df_metrics.Train,4), textangle=0, \n",
    "                xaxis='x2', yaxis='y2', name=\"Train Metrics\")\n",
    "metrics_test = plotly_go.Bar(x=df_metrics.index, y=df_metrics.Test, \n",
    "                showlegend=True, text=round(df_metrics.Test,4), textangle=0, \n",
    "                xaxis='x2', yaxis='y2', name=\"Test Metrics\")\n",
    "\n",
    "f_fig.add_trace(metrics_train, row=1, col=2) \n",
    "f_fig.add_trace(metrics_test, row=1, col=2) \n",
    "\n",
    "train_cm = confusion_matrix(fedot_train.target, f_y_train_pred_optimal, normalize='all')\n",
    "heatmap_train = plotly_go.Heatmap(z=train_cm, \n",
    "                                    x=['0', '1'], y=['0', '1'], \n",
    "                                    colorscale='Blues', \n",
    "                                    text=np.round(train_cm, 3), \n",
    "                                    texttemplate=\"%{text}\", \n",
    "                                    showscale=False)\n",
    "\n",
    "test_cm = confusion_matrix(fedot_test.target, f_y_test_pred_optimal, normalize='all')\n",
    "heatmap_test = plotly_go.Heatmap(z=test_cm, \n",
    "                                    x=['0', '1'], y=['0', '1'], \n",
    "                                    colorscale='Blues', \n",
    "                                    text=np.round(test_cm, 3), \n",
    "                                    texttemplate=\"%{text}\", \n",
    "                                    showscale=False)\n",
    "\n",
    "\n",
    "f_fig.add_trace(heatmap_train, row=2, col=1)\n",
    "f_fig.add_trace(heatmap_test,  row=2, col=2) \n",
    "\n",
    "f_fig.update_layout(\n",
    "    xaxis1 = {'title': 'Predict'},\n",
    "    xaxis2 = {'title': 'Predict'},\n",
    "    yaxis1 = {'title': 'Goals'},\n",
    "    yaxis2 = {'title': 'Goals'},\n",
    "    xaxis3 = {'title': 'Предсказания'},\n",
    "    xaxis4 = {'title': 'Предсказания'},\n",
    "    yaxis3 = {'title': 'Факт'},\n",
    "    yaxis4 = {'title': 'Факт'},\n",
    "                \n",
    ")    \n",
    "f_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fedot_metrics_as_dict = {'params': ModelWrapClass.metrics_names(),\n",
    "                'values': [\n",
    "                    f_train_precision, f_test_precision,\n",
    "                    f_train_recall,    f_test_recall,\n",
    "                    f_train_roc_auc,   f_test_roc_auc,\n",
    "                    f_train_accuracy,  f_test_accuracy,\n",
    "                    f_train_f1_score,  f_test_f1_score\n",
    "                ],\n",
    "                'model_name': [fedot_model_name for i in range(len(ModelWrapClass.metrics_names()))]\n",
    "            }      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 2. Прогнозирование временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить датасет если его нет\n",
    "ts_csv_filename = Path(settings.enviroment[\"DATASET_SUBFOLDER\"], \"\", 'shampoo-sales.csv')\n",
    "if not Path(ts_csv_filename).exists():\n",
    "    if not Path(settings.enviroment[\"DATASET_SUBFOLDER\"]).exists():\n",
    "        Path.mkdir(Path(settings.enviroment[\"DATASET_SUBFOLDER\"]))\n",
    "    od.download_url(\"https://raw.githubusercontent.com/sunilmallya/timeseries/refs/heads/master/data/shampoo-sales.csv\", \n",
    "                                Path(settings.enviroment[\"DATASET_SUBFOLDER\"]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет и для ДЗ отберем 5000 строк из датасета\n",
    "original_dataset_df = pd.read_csv(ts_csv_filename, header=None, names=['date', 'sales'])\n",
    "dataset_df = original_dataset_df.copy()\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнительная таблица метрик разных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = pd.concat([pd.DataFrame(knn_model.metrics()),\n",
    "                     pd.DataFrame(svc_model.metrics()),\n",
    "                     pd.DataFrame(rfc_model.metrics()),\n",
    "                     pd.DataFrame(logreg_model.metrics()),\n",
    "                     pd.DataFrame(dtc_model.metrics()),\n",
    "                     pd.DataFrame(fedot_metrics_as_dict)\n",
    "                     ])\n",
    "columns = ['model_name']\n",
    "columns = columns + ModelClass.metrics_names()\n",
    "df_stat2 = df_stat.pivot_table(columns = 'params',\n",
    "                            index='model_name',\n",
    "                            values='values').reset_index()[columns]\n",
    "df_stat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольшая доля правильных предсказаний (accuracy) у моделей SupportVectorMachine и CatBoost(fedot) - 0.71 на тестовой выборке.\n",
    "Лучше всего положительные классы предстказывает SupportVectorMachine - 0.67.\n",
    "Наиболее сбалансированной получается модель Catboost, выбранная с помощью FEDOT - F1=0.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
